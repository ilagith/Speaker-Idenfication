# -*- coding: utf-8 -*-
"""DL_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11Qw3Jo1T_4z6Si2xacS4bp2OUqK5VqSy
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy
# %pylab inline --no-import-all
import csv
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation
from tensorflow.keras.optimizers import Adam
import numpy as np
import pandas as pd
import random
import itertools
import IPython.display as ipd
import matplotlib.pyplot as plt
from keras.models import load_model
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelBinarizer
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Convolution1D, Convolution2D, MaxPooling2D
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten
from tensorflow.keras.optimizers import Adam
from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from sklearn.preprocessing import StandardScaler
import librosa

#python shared doc to collaborate with ideas and code
 
#importing libraries 
import pickle
import numpy as np
from sklearn.model_selection import train_test_split  
#loading data  
with open('raw_data.pkl', 'rb') as raw_data:
  data = pickle.load(raw_data)

x = data[0]
y = data[1] 

print('Sequences: {}'.format(np.shape((x))))
print('Labels: {}'.format(np.shape((y))))

X_tr, X_te, y_tra, y_te = train_test_split(x, y, test_size = 0.3, random_state = 666, stratify = y)

"""# Training set 
##Data argumentation

"""

# Adding white noise to recording 1 
l = []
for recording in X_tr:
  wn = np.random.randn(len(recording))
  data_wn = recording + 0.005 * wn
  l.append(data_wn)
wn_data = np.array(l).reshape(X_tr.shape)
#ipd.Audio(data_wn, rate=11000)

wn_data[0]

X_tr[0]

# Shifting the sound
l1 = []
for recording in X_tr:
  data_roll = np.roll(recording, 1600)
  l1.append(data_roll)
shift_data = np.array(l1).reshape(X_tr.shape)

shift_data[0]

# stretching the sound
def stretch(data, rate=1):
    input_length = 11025
    data = librosa.effects.time_stretch(data, rate)
    if len(data)>input_length:
        data = data[:input_length]
    else:
        data = np.pad(data, (0, max(0, input_length - len(data))), "constant")

    return data
l2 = []
l3 = []
for recording in X_tr:
  data_stretch1 =stretch(recording, 0.8) #deeper
  l2.append(data_stretch1)
  data_stretch2 =stretch(recording, 1.2) #higher frequency
  l3.append(data_stretch2)
deep_data = np.array(l2).reshape(X_tr.shape)
high_data = np.array(l3).reshape(X_tr.shape)

x_train = np.vstack([X_tr, wn_data, shift_data, deep_data, high_data])

x_train.shape

y_tr = list(y_tra)
len(y_tr)

y_train = y_tr + y_tr + y_tr + y_tr + y_tr 
y_train = np.array(y_train)
y_train.shape

y_train[6720]

"""## Feature Extraction"""

header = 'chroma_stft rmse spectral_centroid spectral_bandwidth rolloff zero_crossing_rate'
for i in range(1, 21):
    header += f' mfcc{i}'
header += ' label'
header = header.split()

file = open('train.csv', 'w', newline='')
with file:
    writer = csv.writer(file)
    writer.writerow(header)
#people = np.unique(y)
for l in range(len(y_train)): 
    chroma_stft = librosa.feature.chroma_stft(y=x_train[l])
    rmse = librosa.feature.rms(y=x_train[l])
    spec_cent = librosa.feature.spectral_centroid(y=x_train[l])
    spec_bw = librosa.feature.spectral_bandwidth(y=x_train[l])
    rolloff = librosa.feature.spectral_rolloff(y=x_train[l])
    zcr = librosa.feature.zero_crossing_rate(x_train[l])
    mfcc = librosa.feature.mfcc(y=x_train[l])
    label = y_train[l]
    to_append = f'{np.mean(chroma_stft)} {np.mean(rmse)} {np.mean(spec_cent)} {np.mean(spec_bw)} {np.mean(rolloff)} {np.mean(zcr)}' 
    for e in mfcc:
        to_append += f' {np.mean(e)}'
    to_append += f' {label}'
    file = open('train.csv', 'a', newline='')
    with file:
        writer = csv.writer(file)
        writer.writerow(to_append.split())

data_train = pd.read_csv('train.csv') #X_train is a df now
data_train.shape

X_train = data_train.iloc[:, :-1]
y_train = data_train.iloc[:, -1]

y_train[6720]

"""#Testing set
## Feature extraction
"""

header = 'chroma_stft rmse spectral_centroid spectral_bandwidth rolloff zero_crossing_rate'
for i in range(1, 21):
    header += f' mfcc{i}'
header += ' label'
header = header.split()

file = open('test.csv', 'w', newline='')
with file:
    writer = csv.writer(file)
    writer.writerow(header)
#people = np.unique(y)
for l in range(len(y_te)): 
    chroma_stft = librosa.feature.chroma_stft(y=X_te[l])
    rmse = librosa.feature.rms(y=X_te[l])
    spec_cent = librosa.feature.spectral_centroid(y=X_te[l])
    spec_bw = librosa.feature.spectral_bandwidth(y=X_te[l])
    rolloff = librosa.feature.spectral_rolloff(y=X_te[l])
    zcr = librosa.feature.zero_crossing_rate(X_te[l])
    mfcc = librosa.feature.mfcc(y=X_te[l])
    label = y_te[l]
    to_append = f'{np.mean(chroma_stft)} {np.mean(rmse)} {np.mean(spec_cent)} {np.mean(spec_bw)} {np.mean(rolloff)} {np.mean(zcr)}' 
    for e in mfcc:
        to_append += f' {np.mean(e)}'
    to_append += f' {label}'
    file = open('test.csv', 'a', newline='')
    with file:
        writer = csv.writer(file)
        writer.writerow(to_append.split())

data_test = pd.read_csv('test.csv') #X_test is a df now
data_test.shape

data_test.head()

X_test = data_test.iloc[:, :-1]
y_test = data_test.iloc[:, -1]

#preprocessing
#standard scale 
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
 
#one-hot encoding 
lb = LabelBinarizer()
Y_train = lb.fit_transform(y_train)
Y_test = lb.transform(y_test)
 
#np arrays dimensions 
X_train = np.array(X_train).reshape(-1, 26, 1)
X_test = np.array(X_test).reshape(-1, 26, 1 )

def residual_block(x, filters, conv_num = 3, activation="tanh"):
   # Shortcut
   s = keras.layers.Conv1D(filters, 5, padding="same")(x)
   for i in range(conv_num - 1):
       x = keras.layers.Conv1D(filters, 5, padding="same")(x)
       x = keras.layers.Activation(activation)(x)
   x = keras.layers.Conv1D(filters, 5, padding="same")(x)
   x = keras.layers.Add()([x, s])
   x = keras.layers.Activation(activation)(x)
   return keras.layers.MaxPool1D(pool_size=1, strides=1)(x)
 
import keras
def build_model(input_shape, num_classes):
  inputs = keras.layers.Input(shape=input_shape, name="input")

  x = residual_block(inputs, 4, 3)
  x = residual_block(x, 8, 2)
  x = residual_block(x, 16, 1)
  x = residual_block(x, 32, 1)
  x = residual_block(x, 32, 3)


  x = keras.layers.AveragePooling1D(pool_size=5, strides=3)(x)
  x = keras.layers.Flatten()(x)
  x = keras.layers.Dropout(0.8)(x) #0.8

  x = keras.layers.Dense(2096, activation="tanh")(x)

  outputs = keras.layers.Dense(num_classes, activation="softmax", name="output")(x)
  

  return keras.models.Model(inputs=inputs, outputs=outputs)
 
 
model = build_model((26, 1), 600)

# Compile the model using Adam's default learning rate
adam = Adam(lr = 0.003)
model.compile(optimizer = adam, loss="categorical_crossentropy", metrics=["accuracy"])
 
# Add callbacks:
# 'EarlyStopping' to stop training when the model is not enhancing anymore
# 'ModelCheckPoint' to always keep the model that has the best val_accuracy
model_save_filename = "model.h5"
 
earlystopping_cb = EarlyStopping(patience=10, restore_best_weights = True)
mdlcheckpoint_cb = ModelCheckpoint(model_save_filename, monitor = "val_accuracy", save_best_only = True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', 
                              factor = 0.2, 
                              patience = 5, 
                              verbose = 1, 
                              mode = 'min', 
                              min_delta =0.0001, 
                              cooldown=3, 
                              min_lr=0)
history = model.fit(
   X_train,
   Y_train,
   epochs = 150,
   #validation_data=(X_val, Y_val),
   validation_split = 0.1,
   shuffle = True,
   batch_size = 512,
   callbacks = [earlystopping_cb, mdlcheckpoint_cb, reduce_lr]
)

from keras.models import load_model
model = load_model('model.h5')
#model.summary()

loss,acc = model.evaluate(X_test, Y_test)
print(loss,acc)

y_pre = model.predict(X_test)
d = []
for i in y_pre: 
  #i = [...], 600 probabilities
  d.append(i) #d is a list of label probability list for each instance

import sklearn.metrics.pairwise
dm = sklearn.metrics.pairwise.cosine_distances(d)
dm.shape

import numpy as np


def calculate_performance_numpy(distances_matrix, labels):
    """
    For a given distance matrix and labels of all samples, this function calculates two performance measures:
     - The mean CMC scores for n = [1, 3, 5, 10]
     - A mean accuracy metric. This metric calculates how many of the k samples that belong to the same class are among
       the first k ranked elements.

    For N samples, the arguments to this function are:
    :param distances_matrix: A NumPy array defining a distance matrix of floats of size [N, N].
    :param labels: An array of integers of size N.

    """
    assert distances_matrix.shape[0] == distances_matrix.shape[1], "The distance matrix must be a square matrix"
    assert len(labels) == distances_matrix.shape[0], "The size of the matrix should be equal to number of labels"

    # Create a bool matrix (mask) where all the elements are True, except for the diagonal.
    mask = np.logical_not(np.eye(labels.shape[0], dtype=np.bool))

    # Create a bool matrix (label_equal) with value True in the position where the row and column (i, j)
    # belong to the same label, except for i = j.
    label_equal = labels[np.newaxis, :] == labels[:, np.newaxis]
    label_equal = np.logical_and(label_equal, mask) #all False

    # Add the maximum distance to the diagonal.
    distances_matrix = distances_matrix + np.logical_not(mask) * np.max(distances_matrix.flatten(), axis=-1)

    # Get the sorted indices of the distance matrix for each sample.
    sorted_indices = np.argsort(distances_matrix, axis=1)

    # Get a bool matrix where the bool values in label_equal are sorted according to sorted_indices
    sorted_equal_labels_all = np.zeros(label_equal.shape, dtype=bool)
    for i, ri in enumerate(sorted_indices):
        sorted_equal_labels_all[i] = label_equal[i][ri]

    # Calculate the mean CMC scores for k=[1, 3, 5, 10] over all samples
    # The score is 1 if a sample j with the same label as i is in the first k ranked positions. It i s 0 otherwise.
    cmc_scores = np.zeros([4])
    for sorted_equal_labels in sorted_equal_labels_all:
        # CMC scores for a sample
        score = np.asarray([np.sum(sorted_equal_labels[:n]) > 0 for n in [1, 3, 5, 10]])
        # Update running average
        cmc_scores = cmc_scores + score
    cmc_scores /= len(sorted_equal_labels_all)

    # Calculate the accuracy metric

    # Calculate how many samples are there with the same label as any sample i.
    num_positives = np.sum(label_equal, axis=1, dtype=np.int)
    num_samples = len(sorted_equal_labels_all)

    # Calculate the average metric by adding up how many labels correspond to sample i in the first n elements of the
    # ranked row. So, if all the first n elements belong to the same labels the sum is n (perfect score).
    acc = 0
    for i, n in enumerate(num_positives): #n = 2, 2, 0, 1....
      acc = acc + np.sum(sorted_equal_labels_all[i, :n], dtype=np.float32) / (n * num_samples)
        
    return cmc_scores, acc

calculate_performance_numpy(dm, y_test)

"""# Task 2"""

with open('test_data.pkl', 'rb') as test_data:
  test = pickle.load(test_data)
test = np.array(test)
test = test.reshape(320, 11025)

#extract features
import csv
import librosa 
header = 'chroma_stft rmse spectral_centroid spectral_bandwidth rolloff zero_crossing_rate '
for i in range(1, 21):
    header += f' mfcc{i}'
#header += ' label'
header = header.split()
file = open('test2.csv', 'w', newline='')
with file:
    writer = csv.writer(file)
    writer.writerow(header)
import librosa
import csv
for l in range(len(test)): 
    chroma_stft = librosa.feature.chroma_stft(y=test[l])
    rmse = librosa.feature.rms(y=test[l])
    spec_cent = librosa.feature.spectral_centroid(y=test[l])
    spec_bw = librosa.feature.spectral_bandwidth(y=test[l])
    rolloff = librosa.feature.spectral_rolloff(y=test[l])
    zcr = librosa.feature.zero_crossing_rate(test[l])
    mfcc = librosa.feature.mfcc(y=test[l])
    #label = data[1][l]
    to_append = f'{np.mean(chroma_stft)} {np.mean(rmse)} {np.mean(spec_cent)} {np.mean(spec_bw)} {np.mean(rolloff)} {np.mean(zcr)}' 
    for e in mfcc:
        to_append += f' {np.mean(e)}'
    #to_append += f' {label}'
    file = open('test2.csv', 'a', newline='')
    with file:
        writer = csv.writer(file)
        writer.writerow(to_append.split())
#load test data extracted

test_feature = pd.read_csv('test2.csv')
test_feature.head()

test_feature = np.array(test_feature)
test_feature.shape

#scale and reshape test 
test_feature = scaler.transform(test_feature)
test_feature = test_feature.reshape(-1, 26, 1)

#predict 
#model = load_model('model.h5')
y_pre_test = model.predict(test_feature)
d_test = []
for i in y_pre_test:
    d_test.append(i)
#cosine distance 
import sklearn.metrics.pairwise
dm_test = sklearn.metrics.pairwise.cosine_distances(d_test)

np.savetxt("answer.txt", dm_test, delimiter=";")